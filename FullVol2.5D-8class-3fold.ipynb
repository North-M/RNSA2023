{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf32a2a-7118-4d81-b093-818e344d05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import pydicom as dicom\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import monai\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from monai.networks.nets import EfficientNetBN\n",
    "from monai.networks.nets import ResNet\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "import timm\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a795ca56-6bcd-4967-b375-dc59214d526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish seeding with seed 344\n",
      "Training on device cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 344\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True # Fix the network according to random seed\n",
    "    print('Finish seeding with seed {}'.format(seed))\n",
    "    \n",
    "seed_everything(SEED)\n",
    "print('Training on device {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a8a495-8bee-478e-a16b-0e27bfd70da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "      <th>any_injury</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10004</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10026</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>9951</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>9960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>9961</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>9980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>9983</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3147 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n",
       "0          10004              1             0                      0   \n",
       "1          10005              1             0                      1   \n",
       "2          10007              1             0                      1   \n",
       "3          10026              1             0                      1   \n",
       "4          10051              1             0                      1   \n",
       "...          ...            ...           ...                    ...   \n",
       "3142        9951              1             0                      1   \n",
       "3143        9960              1             0                      1   \n",
       "3144        9961              1             0                      1   \n",
       "3145        9980              1             0                      1   \n",
       "3146        9983              1             0                      1   \n",
       "\n",
       "      extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n",
       "0                        1               0           1            0   \n",
       "1                        0               1           0            0   \n",
       "2                        0               1           0            0   \n",
       "3                        0               1           0            0   \n",
       "4                        0               1           0            0   \n",
       "...                    ...             ...         ...          ...   \n",
       "3142                     0               1           0            0   \n",
       "3143                     0               1           0            0   \n",
       "3144                     0               1           0            0   \n",
       "3145                     0               1           0            0   \n",
       "3146                     0               1           0            0   \n",
       "\n",
       "      liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n",
       "0                 1          0           0               0           0   \n",
       "1                 1          0           0               1           0   \n",
       "2                 1          0           0               1           0   \n",
       "3                 1          0           0               1           0   \n",
       "4                 1          0           0               0           1   \n",
       "...             ...        ...         ...             ...         ...   \n",
       "3142              1          0           0               1           0   \n",
       "3143              1          0           0               1           0   \n",
       "3144              1          0           0               1           0   \n",
       "3145              1          0           0               0           0   \n",
       "3146              1          0           0               0           0   \n",
       "\n",
       "      spleen_high  any_injury  \n",
       "0               1           1  \n",
       "1               0           0  \n",
       "2               0           0  \n",
       "3               0           0  \n",
       "4               0           1  \n",
       "...           ...         ...  \n",
       "3142            0           0  \n",
       "3143            0           0  \n",
       "3144            0           0  \n",
       "3145            1           1  \n",
       "3146            1           1  \n",
       "\n",
       "[3147 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom_tag_columns = [\n",
    "    'Columns',\n",
    "    'ImageOrientationPatient',\n",
    "    'ImagePositionPatient',\n",
    "    'InstanceNumber',\n",
    "    'PatientID',\n",
    "    'PatientPosition',\n",
    "    'PixelSpacing',\n",
    "    'RescaleIntercept',\n",
    "    'RescaleSlope',\n",
    "    'Rows',\n",
    "    'SeriesNumber',\n",
    "    'SliceThickness',\n",
    "    'path',\n",
    "    'WindowCenter',\n",
    "    'WindowWidth'\n",
    "]\n",
    "\n",
    "train_dicom_tags = pd.read_parquet('autodl-tmp/train_dicom_tags.parquet', columns=dicom_tag_columns)\n",
    "test_dicom_tags = pd.read_parquet('autodl-tmp/test_dicom_tags.parquet', columns=dicom_tag_columns)\n",
    "\n",
    "train_series_meta = pd.read_csv('autodl-tmp/train_series_meta.csv')\n",
    "test_series_meta = pd.read_csv('autodl-tmp/test_series_meta.csv')\n",
    "\n",
    "train_csv = pd.read_csv('autodl-tmp/train.csv')\n",
    "\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b75b036-35cc-4f07-9480-1d2ff2ab6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_series_meta = train_series_meta.loc[train_series_meta.patient_id.isin(train_csv.loc[train_csv.any_injury == 1, \"patient_id\"].values)]\n",
    "healthy_series_meta = train_series_meta.loc[train_series_meta.patient_id.isin(train_csv.loc[train_csv.any_injury == 0, \"patient_id\"].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7096cd30-8868-4b07-b313-ea7cfce8fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_path_gen(patient_id, series_id, train=True):\n",
    "    if(train):\n",
    "        path = 'autodl-tmp/train_images_resample/'\n",
    "    else:\n",
    "        path = 'autodl-tmp/train_images_resample/'\n",
    "    \n",
    "    path += str(patient_id) + '/' + str(series_id)\n",
    "    \n",
    "    return path\n",
    "\n",
    "def create_3D_scans(folder, downsample_rate=1): \n",
    "    filenames = os.listdir(folder)\n",
    "    filenames = [int(filename.split('.')[0]) for filename in filenames]\n",
    "    filenames = sorted(filenames)\n",
    "    filenames = [str(filename) + '.dcm' for filename in filenames]\n",
    "        \n",
    "    volume = []\n",
    "    #for filename in tqdm(filenames[::downsample_rate], position=0): \n",
    "    for filename in filenames[::downsample_rate]: \n",
    "        filepath = os.path.join(folder, filename)\n",
    "        ds = dicom.dcmread(filepath)\n",
    "        image = ds.pixel_array\n",
    "        \n",
    "        if ds.PixelRepresentation == 1:\n",
    "            bit_shift = ds.BitsAllocated - ds.BitsStored\n",
    "            dtype = image.dtype \n",
    "            image = (image << bit_shift).astype(dtype) >>  bit_shift\n",
    "        \n",
    "        # find rescale params\n",
    "        if (\"RescaleIntercept\" in ds) and (\"RescaleSlope\" in ds):\n",
    "            intercept = float(ds.RescaleIntercept)\n",
    "            slope = float(ds.RescaleSlope)\n",
    "    \n",
    "        # find clipping params\n",
    "        center = int(ds.WindowCenter)\n",
    "        width = int(ds.WindowWidth)\n",
    "        low = center - width / 2\n",
    "        high = center + width / 2    \n",
    "        \n",
    "        \n",
    "        image = (image * slope) + intercept\n",
    "        image = np.clip(image, low, high)\n",
    "\n",
    "        image = (image / np.max(image) * 255).astype(np.int16)\n",
    "        image = image[::downsample_rate, ::downsample_rate]\n",
    "        volume.append( image )\n",
    "    \n",
    "    volume = np.stack(volume, axis=0)\n",
    "    return volume\n",
    "\n",
    "def plot_image_with_seg(volume, volume_seg=[], orientation='Coronal', num_subplots=20):\n",
    "    # simply copy\n",
    "    if len(volume_seg) == 0:\n",
    "        plot_mask = 0\n",
    "    else:\n",
    "        plot_mask = 1\n",
    "        \n",
    "    if orientation == 'Coronal':\n",
    "        slices = np.linspace(0, volume.shape[2]-1, num_subplots).astype(np.int16)\n",
    "        volume = volume.transpose([1, 0, 2])\n",
    "        if plot_mask:\n",
    "            volume_seg = volume_seg.transpose([1, 0, 2])\n",
    "        \n",
    "    elif orientation == 'Sagittal':\n",
    "        slices = np.linspace(0, volume.shape[2]-1, num_subplots).astype(np.int16)\n",
    "        volume = volume.transpose([2, 0, 1])\n",
    "        if plot_mask:\n",
    "            volume_seg = volume_seg.transpose([2, 0, 1])\n",
    "\n",
    "    elif orientation == 'Axial':\n",
    "        slices = np.linspace(0, volume.shape[0]-1, num_subplots).astype(np.int16)\n",
    "           \n",
    "    rows = np.max( [np.floor(np.sqrt(num_subplots)).astype(int) - 2, 1])\n",
    "    cols = np.ceil(num_subplots/rows).astype(int)\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(cols * 2, rows * 4))\n",
    "    fig.tight_layout(h_pad=0.01, w_pad=0)\n",
    "    \n",
    "    ax = ax.ravel()\n",
    "    for this_ax in ax:\n",
    "        this_ax.axis('off')\n",
    "\n",
    "    for counter, this_slice in enumerate( slices ):\n",
    "        plt.sca(ax[counter])\n",
    "        \n",
    "        image = volume[this_slice, :, :]\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        \n",
    "        if plot_mask:\n",
    "            mask = np.where(volume_seg[this_slice, :, :], volume_seg[this_slice, :, :], np.nan)\n",
    "            plt.imshow(mask, cmap='Set1', alpha=0.5)\n",
    "            \n",
    "def load_nii(patient_id, series_id, root='autodl-tmp/train_images_resample/'):\n",
    "    path = root + str(patient_id) + '/' + str(series_id) + '.nii.gz'\n",
    "    img = sitk.ReadImage(path)\n",
    "    img = sitk.GetArrayFromImage(img)\n",
    "    \n",
    "    # img = nib.load(path)\n",
    "    # img = img.get_fdata().transpose(2, 1, 0)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2141f4ef-37c8-4cef-b89d-7ff9a539c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = 2\n",
    "\n",
    "class CTDataset(Dataset):\n",
    "    def __init__(self, root='autodl-tmp/train_images_resample/', augmentation=False, meta=train_series_meta, device='cpu'):\n",
    "        self.device = device\n",
    "        self.series_meta = meta\n",
    "        self.root = root\n",
    "        self.t = monai.transforms.Compose([monai.transforms.RandZoom(prob=0.5, min_zoom=0.9, max_zoom=1.1),\n",
    "                                           monai.transforms.RandRotate(range_x=3.14 / 24, prob=0.5),\n",
    "                                           monai.transforms.SpatialPad(spatial_size=(320//ds, 280//ds, 280//ds), mode=\"edge\"),\n",
    "                                           monai.transforms.RandSpatialCrop(roi_size=(320//ds, 256//ds, 256//ds), random_size=False),\n",
    "                                           monai.transforms.NormalizeIntensity(divisor = 400)\n",
    "                                ])\n",
    "        self.t_val = monai.transforms.Compose([monai.transforms.NormalizeIntensity(divisor = 400)])\n",
    "        \n",
    "        self.aug = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return 1100\n",
    "        return len(self.series_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        patient_id, series_id = self.series_meta.loc[idx, [\"patient_id\", \"series_id\"]].astype('int')\n",
    "        img_a = load_nii(patient_id, series_id, self.root).astype('float32')\n",
    "        #img_t = torch.from_numpy(img_a).unsqueeze(0)\n",
    "        #img_t = torch.from_numpy(img_a[::2, ::2, ::2]).unsqueeze(0)\n",
    "        if(self.aug):\n",
    "            img_t = self.t(np.expand_dims(img_a[::ds, ::ds, ::ds], 0))\n",
    "        else:\n",
    "            #img_t = torch.from_numpy(img_a[::ds, ::ds, ::ds]).unsqueeze(0)\n",
    "            img_t = self.t_val(np.expand_dims(img_a[::ds, ::ds, ::ds], 0))\n",
    "        label_columns = [\n",
    "            'bowel_injury',\n",
    "            'extravasation_injury',\n",
    "            'kidney_low',\n",
    "            'kidney_high',\n",
    "            'liver_low',\n",
    "            'liver_high',\n",
    "            'spleen_low',\n",
    "            'spleen_high',\n",
    "            #'any_injury'\n",
    "        ]\n",
    "        label_a = train_csv.loc[train_csv.patient_id == patient_id, label_columns].values[0].astype('float32')\n",
    "        label_t = torch.from_numpy(label_a)\n",
    "        return img_t, label_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd90d76-a251-4d2d-9667-6b4c23550672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_meta = train_series_meta[-3600:].reset_index()\n",
    "# val_meta = train_series_meta[0:-3600].reset_index()\n",
    "\n",
    "# # train_meta = injury_series_meta[-800:].reset_index()\n",
    "# # val_meta = injury_series_meta[0:-800].reset_index()\n",
    "\n",
    "# train_ds = CTDataset(meta = train_meta, augmentation=True)\n",
    "# train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=8)\n",
    "\n",
    "# val_ds = CTDataset(meta = val_meta, augmentation=False)\n",
    "# val_dl = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1f0a1d-e6aa-4575-b1f1-5efd4457a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EffNet(nn.Module):\n",
    "#     def __init__(self, ch_out=9):\n",
    "#         super(EffNet, self).__init__()\n",
    "#         #self.conv_in = nn.Conv3d(1, 3, kernel_size=5, padding=2, stride=2)\n",
    "#         self.net = EfficientNetBN(\"efficientnet-b0\", pretrained=False, progress=False, spatial_dims=3, in_channels=1, num_classes=ch_out,)\n",
    "#     def forward(self, x):\n",
    "#         #x = self.conv_in(x)\n",
    "#         #return torch.sigmoid(self.net(x))\n",
    "#         return self.net(x)\n",
    "\n",
    "class FullVolNet(nn.Module):\n",
    "    def __init__(self, backbone = \"tf_efficientnetv2_s.in21k_ft_in1k\", \n",
    "                 ch_in = 3, ch_out = 9, slices = 15, dropout = 0.0, pretrained=True):\n",
    "        super(FullVolNet, self).__init__()\n",
    "        self.slices = slices\n",
    "        \n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=ch_in,\n",
    "            num_classes=ch_out,\n",
    "            features_only=False,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            pretrained=False,\n",
    "        )\n",
    "        \n",
    "        if 'efficient' in backbone and pretrained:\n",
    "            self.encoder.load_state_dict(torch.load('pretrained/tf_efficientnetv2_s.in21k_ft_in1k_8class.pt'))\n",
    "        elif 'convnext' in backbone and pretrained:\n",
    "            self.encoder.load_state_dict(torch.load('pretrained/convnextv2_nano.fcmae_ft_in22k_in1k_384_8class.pt'))\n",
    "        \n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "        \n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0.0, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, ch_out),\n",
    "        )\n",
    "        \n",
    "        self.head2 = nn.Conv1d(slices, 1, 1)\n",
    "        \n",
    "        \n",
    "    def slicer(self, img, slices):\n",
    "        #img = img.squeeze(1)\n",
    "        z_length = img.shape[-3]\n",
    "        z_slices = (np.linspace(0, z_length, slices + 4)).astype('int')\n",
    "        z_slices = z_slices[2:-2]\n",
    "        #print(z_slices)\n",
    "        slices_list = []\n",
    "        for z in z_slices:\n",
    "            slices_list.append(img[:, :, z-1:z+2, :, :])\n",
    "        img_slice = torch.cat(slices_list, 1)\n",
    "        return img_slice\n",
    "        \n",
    "    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n",
    "        x = self.slicer(x, self.slices)\n",
    "        bs, nslice,ch, sz1, sz2 = x.shape\n",
    "        x = x.view(bs*nslice, ch, sz1, sz2)\n",
    "        \n",
    "        feature_2d = self.encoder(x)\n",
    "        feature_2d = feature_2d.view(bs, nslice, -1)\n",
    "        \n",
    "        feature_lstm, _ = self.lstm(feature_2d)\n",
    "        feature_lstm = feature_lstm.contiguous().view(bs * nslice, -1)\n",
    "        \n",
    "        preds = self.head(feature_lstm)\n",
    "        preds = preds.view(bs, nslice, -1).contiguous()\n",
    "        preds = self.head2(preds)\n",
    "        \n",
    "        return preds.squeeze(1)\n",
    "        \n",
    "        \n",
    "        # bs = x.shape[0]\n",
    "        # x = x.view(bs * n_slice_per_c, in_chans, image_size, image_size)\n",
    "        # feat = self.encoder(x)\n",
    "        # feat = feat.view(bs, n_slice_per_c, -1)\n",
    "        # feat, _ = self.lstm(feat)\n",
    "        # feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n",
    "        # feat = self.head(feat)\n",
    "        # feat = feat.view(bs, n_slice_per_c).contiguous()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "452b48a5-36e6-4c16-967c-5b5e151c5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avail_pretrained_models = timm.list_models(pretrained=True)\n",
    "# avail_pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b95491b-07eb-45ea-90f6-456377057226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs, labels = next(iter(train_dl))\n",
    "# net = FullVolNet(ch_out = 13)\n",
    "# img_s = net(imgs)\n",
    "# img_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af7441f-f99e-44ef-8e67-11273ff4ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def transform_9class(label_in):\n",
    "    label_out = [1 - label_in[0],\n",
    "                   label_in[0],\n",
    "                    1- label_in[1],\n",
    "                   label_in[1],\n",
    "                   (1 - label_in[2]) * (1 - label_in[3]),\n",
    "                   label_in[2],\n",
    "                   label_in[3],\n",
    "                   (1 - label_in[4]) * (1 - label_in[5]),\n",
    "                    label_in[4],\n",
    "                   label_in[5],\n",
    "                   (1 - label_in[6]) * (1 - label_in[7]),\n",
    "                   label_in[6],\n",
    "                   label_in[7]]\n",
    "    return label_out\n",
    "\n",
    "def transform_13class(label_in):\n",
    "    label_out = label_in\n",
    "    return label_out.tolist()\n",
    "\n",
    "\n",
    "def loss_metrics(metrics, transform):\n",
    "    preds = [transform(x) for x in metrics[\"predict\"]]\n",
    "    targets = [transform(x) for x in metrics[\"label\"]]\n",
    "    targets_any_injury = metrics[\"label\"][:, -1]\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    print(\"F1 score: \", sklearn.metrics.f1_score(metrics[\"label\"], np.around(metrics[\"predict\"]), average=None, zero_division=0.0))\n",
    "    print(\"AUC score: \", sklearn.metrics.roc_auc_score(metrics[\"label\"], metrics[\"predict\"], average=None))\n",
    "    \n",
    "    for i in range(0, len(preds)):\n",
    "        predict = preds[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        label_pred = np.zeros(14)\n",
    "        label_pred[0] = predict[0] / (predict[0] + predict[1])\n",
    "        label_pred[1] = predict[1] / (predict[0] + predict[1])\n",
    "        label_pred[2] = predict[2] / (predict[2] + predict[3])\n",
    "        label_pred[3] = predict[3] / (predict[2] + predict[3])\n",
    "        label_pred[4] = predict[4] / (predict[4] + predict[5] + predict[6])\n",
    "        label_pred[5] = predict[5] / (predict[4] + predict[5] + predict[6])\n",
    "        label_pred[6] = predict[6] / (predict[4] + predict[5] + predict[6])\n",
    "        label_pred[7] = predict[7] / (predict[7] + predict[8] + predict[9])\n",
    "        label_pred[8] = predict[8] / (predict[7] + predict[8] + predict[9])\n",
    "        label_pred[9] = predict[9] / (predict[7] + predict[8] + predict[9])\n",
    "        label_pred[10] = predict[10] / (predict[10] + predict[11] + predict[12])\n",
    "        label_pred[11] = predict[11] / (predict[10] + predict[11] + predict[12])\n",
    "        label_pred[12] = predict[12] / (predict[10] + predict[11] + predict[12])\n",
    "        label_pred[13] = max([1 - label_pred[x] for x in [0, 2, 4, 7, 10]])\n",
    "        \n",
    "        targets_any_injury = max([1 - target[x] for x in [0, 2, 4, 7, 10]])\n",
    "        \n",
    "        target.append(targets_any_injury)\n",
    "        label_target = np.array(target)\n",
    "        \n",
    "        weight = np.array([1, 2, 1, 6, 1, 2, 4, 1, 2, 4, 1, 2, 4, 6])\n",
    "        \n",
    "        loss_list.append(sklearn.metrics.log_loss(\n",
    "            y_true=label_target,\n",
    "            y_pred=label_pred,\n",
    "            sample_weight=weight))\n",
    "    #print(\"Weighted Loss: \" + np.mean(loss_list))\n",
    "    \n",
    "    return np.mean(loss_list)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #print(np.array(preds).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc4c8d8-bcaf-457a-a88e-48e6d3bdb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def TrainClassifer(model,trn_dl,val_dl,optimizer, project, name, suffix, scheduler=None,\n",
    "                   n_eopchs=20, device='cpu'):\n",
    " \n",
    "    #loss_fn = nn.BCELoss(weight=torch.Tensor([1, 6, 1, 6, 1, 4, 8, 1, 4, 8, 1, 4, 8]).to(device))\n",
    "    #loss_fn = nn.BCELoss()\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([10, 5, 10, 10, 6, 6, 6, 6]).to(device))\n",
    "    model.to(device)\n",
    "    best_model = copy.deepcopy(model)\n",
    "    bestweight_model = copy.deepcopy(model)\n",
    "    best_val = 999.0\n",
    "    best_weightloss = 999.0\n",
    "    metrics = {'predict': [], 'label' : []}\n",
    "    PATH_MODEL = project + '/' + name + '/' + suffix + '.pt'\n",
    "    wandb.init(name=name, \n",
    "               project=project)\n",
    "\n",
    "    for epoch in range(1, n_eopchs + 1):\n",
    "        loss_train = 0.0\n",
    "        model.train()\n",
    "        for imgs, labels in tqdm(trn_dl, position=0):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            #outputs = model(imgs.unsqueeze(1))\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        loss_val = 0.0\n",
    "        correct_val = 0.0\n",
    "        model.eval()\n",
    "        \n",
    "        for imgs, labels in tqdm(val_dl):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(imgs)\n",
    "                #outputs = model(imgs.unsqueeze(1))\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss_val += loss.item()\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                metrics['predict'].extend((outputs.to('cpu').detach().numpy()).tolist())\n",
    "                metrics['label'].extend((labels.to('cpu').detach().numpy()).tolist())\n",
    "        \n",
    "        metrics['predict'] = np.array(metrics['predict'])\n",
    "        #metrics['predict'] = np.array([[0.5, 0.5, 0.33333, 0.33333, 0.33333, 0.33333, 0.33333, 0.33333, 0.33333]]*len(metrics['label']))\n",
    "        metrics['label'] = np.array(metrics['label'])\n",
    "        weighted_loss = loss_metrics(metrics, transform_9class)\n",
    "        metrics = {'predict': [], 'label' : []}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if (weighted_loss) < best_weightloss:\n",
    "            best_weightloss = weighted_loss\n",
    "            \n",
    "        if loss_val / len(val_dl) < best_val:\n",
    "            best_val = loss_val / len(val_dl)\n",
    "            torch.save(model.state_dict(), 'model_tmp.pt')\n",
    "            best_model.load_state_dict(torch.load('model_tmp.pt'))\n",
    "            \n",
    "            \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print('{} Eopch {}, Training Loss {}, Val Loss {}, Weighted Loss {}'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()),\n",
    "                                                                  epoch, loss_train / len(trn_dl), loss_val / len(val_dl),\n",
    "                                                                                     weighted_loss))\n",
    "        \n",
    "        \n",
    "        \n",
    "        wandb.log({'training loss': loss_train / len(trn_dl),\n",
    "                  'val loss': loss_val / len(val_dl),\n",
    "                  'weighted loss': weighted_loss})\n",
    "    torch.save(best_model.state_dict(), PATH_MODEL)\n",
    "    print('Finish training: best_val:{}, best_weighted loss:{}'.format(best_val, best_weightloss))\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f48c049d-7037-4df4-a926-0eb5b79be3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project = \"FullVol2.5D-nophase-4fold\"\n",
    "name = \"FullVol2.5D-nophase_Convnetv2_4fold\"\n",
    "# backbone = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "backbone = 'convnextv2_nano.fcmae_ft_in22k_in1k_384'\n",
    "n_eopchs = 15\n",
    "\n",
    "p = project + '/' + name\n",
    "if not os.path.exists(p):\n",
    "    os.mkdir(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5c0d04-a4bf-460c-8f23-3e2eda0c2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4711 // 4 = 1178\n",
    "\n",
    "train_df_fold1 = train_series_meta[1178:].reset_index(drop=True)\n",
    "train_df_fold2 = pd.concat([train_series_meta[0:1178], train_series_meta[2356:]], axis=0).reset_index(drop=True)\n",
    "train_df_fold3 = pd.concat([train_series_meta[0:2356], train_series_meta[3534:]], axis=0).reset_index(drop=True)\n",
    "train_df_fold4 = train_series_meta[:3534].reset_index()\n",
    "\n",
    "val_df_fold1 = train_series_meta[0:1178].reset_index(drop=True)\n",
    "val_df_fold2 = train_series_meta[1178:2356].reset_index(drop=True)\n",
    "val_df_fold3 = train_series_meta[2356:3534].reset_index(drop=True)\n",
    "val_df_fold4 = train_series_meta[3534:].reset_index(drop=True)\n",
    "\n",
    "train_ds_fold1 = CTDataset(meta = train_df_fold1, augmentation=True)\n",
    "train_dl_fold1 = DataLoader(train_ds_fold1, batch_size=4, shuffle=True, num_workers=8)\n",
    "val_ds_fold1 = CTDataset(meta = val_df_fold1, augmentation=False)\n",
    "val_dl_fold1 = DataLoader(val_ds_fold1, batch_size=4, shuffle=False, num_workers=8)\n",
    "\n",
    "train_ds_fold2 = CTDataset(meta = train_df_fold2, augmentation=True)\n",
    "train_dl_fold2 = DataLoader(train_ds_fold2, batch_size=4, shuffle=True, num_workers=8)\n",
    "val_ds_fold2 = CTDataset(meta = val_df_fold2, augmentation=False)\n",
    "val_dl_fold2 = DataLoader(val_ds_fold2, batch_size=4, shuffle=False, num_workers=8)\n",
    "\n",
    "train_ds_fold3 = CTDataset(meta = train_df_fold3, augmentation=True)\n",
    "train_dl_fold3 = DataLoader(train_ds_fold3, batch_size=4, shuffle=True, num_workers=8)\n",
    "val_ds_fold3 = CTDataset(meta = val_df_fold3, augmentation=False)\n",
    "val_dl_fold3 = DataLoader(val_ds_fold3, batch_size=4, shuffle=False, num_workers=8)\n",
    "\n",
    "train_ds_fold4 = CTDataset(meta = train_df_fold4, augmentation=True)\n",
    "train_dl_fold4 = DataLoader(train_ds_fold4, batch_size=4, shuffle=True, num_workers=8)\n",
    "val_ds_fold4 = CTDataset(meta = val_df_fold4, augmentation=False)\n",
    "val_dl_fold4 = DataLoader(val_ds_fold4, batch_size=4, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f62feec-1210-471d-85e7-43da198715bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [01:14<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  [0.19512195 0.23668639 0.15469613 0.30434783 0.27195467 0.09230769\n",
      " 0.20408163 0.32467532]\n",
      "AUC score:  [0.73890339 0.72617573 0.69875816 0.86453416 0.68637631 0.77386633\n",
      " 0.67781067 0.82793679]\n",
      "0.6219838253901167\n"
     ]
    }
   ],
   "source": [
    "# model = FullVolNet(backbone=\"tf_efficientnetv2_s.in21k_ft_in1k\", ch_out = 8).to(device)\n",
    "# model.load_state_dict(torch.load('FullVol2.5D-nophase-4fold/FullVol2.5D-nophase_Effnetv2_4fold/fold1.pt'))\n",
    "# model.eval()\n",
    "# loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([10, 5, 10, 10, 6, 6, 6, 6]).to(device))\n",
    "# val_dl = val_dl_fold1\n",
    "# metrics = {'predict': [], 'label' : []}\n",
    "# loss_val = 0.0\n",
    "# for imgs, labels in tqdm(val_dl):\n",
    "#     imgs = imgs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(imgs)\n",
    "#         #outputs = model(imgs.unsqueeze(1))\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss_val += loss.item()\n",
    "#         outputs = torch.sigmoid(outputs)\n",
    "#         metrics['predict'].extend((outputs.to('cpu').detach().numpy()).tolist())\n",
    "#         metrics['label'].extend((labels.to('cpu').detach().numpy()).tolist())\n",
    "\n",
    "# metrics['predict'] = np.array(metrics['predict'])\n",
    "# #metrics['predict'] = np.array([[0.5, 0.5, 0.33333, 0.33333, 0.33333, 0.33333, 0.33333, 0.33333, 0.33333]]*len(metrics['label']))\n",
    "# metrics['label'] = np.array(metrics['label'])\n",
    "# weighted_loss = loss_metrics(metrics, transform_9class)\n",
    "# print(loss_val / len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d66c5194-5055-4026-b7e2-80458b422525",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FullVolNet(backbone=backbone, ch_out = 8).to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-5)\n",
    "TrainClassifer(model=net,trn_dl=train_dl_fold1,val_dl=val_dl_fold1,optimizer=optimizer, \n",
    "               project=project, name=name, suffix='fold1', scheduler=None, n_eopchs=n_eopchs, device=device)\n",
    "\n",
    "net = FullVolNet(backbone=backbone, ch_out = 8).to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-5)\n",
    "TrainClassifer(model=net,trn_dl=train_dl_fold2,val_dl=val_dl_fold2,optimizer=optimizer, \n",
    "               project=project, name=name, suffix='fold2', scheduler=None, n_eopchs=n_eopchs, device=device)\n",
    "\n",
    "net = FullVolNet(backbone=backbone, ch_out = 8).to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-5)\n",
    "TrainClassifer(model=net,trn_dl=train_dl_fold3,val_dl=val_dl_fold3,optimizer=optimizer, \n",
    "               project=project, name=name, suffix='fold3', scheduler=None, n_eopchs=n_eopchs, device=device)\n",
    "\n",
    "net = FullVolNet(backbone=backbone, ch_out = 8).to(device)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-5)\n",
    "TrainClassifer(model=net,trn_dl=train_dl_fold4,val_dl=val_dl_fold4,optimizer=optimizer, \n",
    "               project=project, name=name, suffix='fold4', scheduler=None, n_eopchs=n_eopchs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "732ea298-acdd-451a-8881-b6f393a121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #net = FullVolNet(backbone = 'convnextv2_nano.fcmae_ft_in22k_in1k_384', ch_out = 8, dropout = 0.5).to(device)\n",
    "# net = FullVolNet(ch_out = 8).to(device)\n",
    "\n",
    "# optimizer = optim.AdamW(net.parameters(), lr=1e-5)\n",
    "# TrainClassifer(model=net,trn_dl=train_dl,val_dl=val_dl,optimizer=optimizer, \n",
    "#                scheduler=None, n_eopchs=15, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea3821-71ab-4773-84c4-cacfabba3cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
